{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/huerd/GPA659-E2020/blob/howard%2Fcontinue/_project/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "de3vKs3GLq1k",
        "colab_type": "text"
      },
      "source": [
        "### *Resources*\n",
        "\n",
        "### Visualisations\n",
        "*   Image Files to Numpy Array : https://www.kaggle.com/lgmoneda/from-image-files-to-numpy-arrays\n",
        "*   MobileNetV2 Dogs/Cats implementation : https://www.kaggle.com/abdallahhassan/dogs-cats-mobilenetv2-transfere-learning\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lor2vxI9tue",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# Runtime Setup : Authorization w/ Kaggle database and download\n",
        "\n",
        "1.   Set your Kaggle API .JSON file to this runtime\n",
        "2.   Downloads and extract dogs-vs-cats.zip into two folders\n",
        "3.   Installs/replaces other programs\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9NfrRKqqTI1",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": "OK"
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "outputId": "0b5c8646-7d6e-49ba-ba00-3bfcb7065b19"
      },
      "source": [
        "# updates kaggle version\n",
        "!pip install pillow==7.0.0 \n",
        "!pip install --upgrade --force-reinstall --no-deps kaggle\n",
        "\n",
        "# fix https://github.com/keras-team/keras-preprocessing/issues/116 for datagen\n",
        "!pip uninstall -y keras-preprocessing\n",
        "!pip install git+https://github.com/keras-team/keras-preprocessing.git\n",
        "\n",
        "from google.colab import files\n",
        "import zipfile\n",
        "\n",
        "# select your Kaggle API kaggle.json\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))\n",
        "  \n",
        "# Then move kaggle.json into the folder where the API expects to find it.\n",
        "!mkdir -p ~/.kaggle/ && mv kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# make sure to create a kaggle account, get it verified (phone number and all)\n",
        "# then go accept terms here : https://www.kaggle.com/c/dogs-vs-cats\n",
        "\n",
        "# zip should be downloaded to /content/\n",
        "!kaggle competitions download -c dogs-vs-cats\n",
        "unzipMaster = zipfile.ZipFile(\"dogs-vs-cats.zip\", 'r')\n",
        "unzipMaster.extractall()\n",
        "unzipMaster.close()\n",
        "\n",
        "# don't expand the folders unless you want colab to crash on you\n",
        "# file format is 1.jpg, 2.jpg, etc\n",
        "unzipTest1 = zipfile.ZipFile(\"test1.zip\", 'r')\n",
        "unzipTest1.extractall()\n",
        "unzipTest1.close()\n",
        "\n",
        "# file format is cat.0.jpg, dog.2.jpg, etc\n",
        "unzipTrain1 = zipfile.ZipFile(\"train.zip\", 'r')\n",
        "unzipTrain1.extractall()\n",
        "unzipTrain1.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pillow==7.0.0 in /usr/local/lib/python3.6/dist-packages (7.0.0)\n",
            "Processing /root/.cache/pip/wheels/57/4e/e8/bb28d035162fb8f17f8ca5d42c3230e284c6aa565b42b72674/kaggle-1.5.6-cp36-none-any.whl\n",
            "Installing collected packages: kaggle\n",
            "  Found existing installation: kaggle 1.5.6\n",
            "    Uninstalling kaggle-1.5.6:\n",
            "      Successfully uninstalled kaggle-1.5.6\n",
            "Successfully installed kaggle-1.5.6\n",
            "Uninstalling Keras-Preprocessing-1.1.2:\n",
            "  Successfully uninstalled Keras-Preprocessing-1.1.2\n",
            "Collecting git+https://github.com/keras-team/keras-preprocessing.git\n",
            "  Cloning https://github.com/keras-team/keras-preprocessing.git to /tmp/pip-req-build-so3dhukn\n",
            "  Running command git clone -q https://github.com/keras-team/keras-preprocessing.git /tmp/pip-req-build-so3dhukn\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from Keras-Preprocessing==1.1.2) (1.18.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras-Preprocessing==1.1.2) (1.15.0)\n",
            "Building wheels for collected packages: Keras-Preprocessing\n",
            "  Building wheel for Keras-Preprocessing (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for Keras-Preprocessing: filename=Keras_Preprocessing-1.1.2-cp36-none-any.whl size=42983 sha256=2ccab94c1ac5be432ffc6ae74cc0a12a99ab303b3367438e913edcab49bc2293\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-9g0i4xvo/wheels/03/a0/39/171f6040d36f36c71168dc69afa81334351b20955dc36ce932\n",
            "Successfully built Keras-Preprocessing\n",
            "Installing collected packages: Keras-Preprocessing\n",
            "Successfully installed Keras-Preprocessing-1.1.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "keras_preprocessing"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-7ec43533-92e5-4dc9-a84f-f34a33286ae7\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-7ec43533-92e5-4dc9-a84f-f34a33286ae7\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5W_wJQ3H-jAn",
        "colab_type": "text"
      },
      "source": [
        "# Dev Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2OB3uf8m3po8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "# uncomment below to use version 1.x\n",
        "#%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "from keras.applications import mobilenet, MobileNetV2\n",
        "from keras.models import Model, Sequential\n",
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "from keras.layers import Input, Dense, Conv2D, Conv3D, DepthwiseConv2D, SeparableConv2D, Conv3DTranspose\n",
        "from keras.layers import Flatten, MaxPool2D, AvgPool2D, GlobalAvgPool2D, UpSampling2D, BatchNormalization\n",
        "from keras.layers import Concatenate, Add, Dropout, ReLU, Lambda, Activation, LeakyReLU, PReLU\n",
        "\n",
        "from scipy import ndimage\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas\n",
        "import numpy\n",
        "import os, sys\n",
        "import IPython.display\n",
        "import PIL.Image\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSu0MebZVd_s",
        "colab_type": "text"
      },
      "source": [
        "## Constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXX2vwNFVdY4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "IMAGE_WIDTH = 150\n",
        "IMAGE_LENGTH = 150\n",
        "IMAGE_SIZE = [IMAGE_WIDTH, IMAGE_LENGTH]\n",
        "IMAGE_NUMCHANNELS = 3\n",
        "\n",
        "TRAINING_SAMPLES = 8000\n",
        "TRAININGDATASPLIT_RATIO = 0.9\n",
        "BATCH_SIZE = 32\n",
        "SEED = 4\n",
        "\n",
        "QUICK_TRAIN = False\n",
        "EPOCH_QUICK = 2\n",
        "EPOCH_NORMAL = 200\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iI1MKysYTZgD",
        "colab_type": "text"
      },
      "source": [
        "# Data Preperation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWT6mAyfzNWx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainingData_dir = \"train/\"\n",
        "testingData_dir = \"test1/\"\n",
        "\n",
        "# load data\n",
        "labeledDataset = os.listdir(trainingData_dir)\n",
        "testingDataset = os.listdir(testingData_dir)\n",
        "\n",
        "results = []\n",
        "\n",
        "# loop through our labeledDataset, one file at a time\n",
        "for eachImage in labeledDataset:\n",
        "  # recall that our labeledDataset file format is [animal].[num].jpg\n",
        "  petType = eachImage.split('.')[0]\n",
        "  if petType == 'cat':\n",
        "    results.append(\"cat\")\n",
        "  else:\n",
        "    results.append(\"dog\")\n",
        "\n",
        "# df_labeled is where all our labeled data is stored in a DataFrame\n",
        "df_labeled = pandas.DataFrame({\n",
        "    'imageName': labeledDataset,\n",
        "    'petType': results\n",
        "})\n",
        "\n",
        "## Visualizations\n",
        "# create figure size\n",
        "plt.figure(figsize=(20, 12))\n",
        "for i in range(0, 6):\n",
        "    plt.subplot(1, 6, i+1)\n",
        "    imageName = random.choice(labeledDataset)\n",
        "    image = load_img(trainingData_dir + imageName)\n",
        "    plt.title(imageName)\n",
        "    plt.imshow(image)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Total Counts from Labeled Data\")\n",
        "df_labeled['petType'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBzml4WzUaBI",
        "colab_type": "text"
      },
      "source": [
        "## Training Data allocation\n",
        "\n",
        "We ratio the labeled training data into two sets\n",
        "* Data used to train our model (training data)\n",
        "* Data used to validate our model (validation data)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BQ5XcXdUZ52",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# split your labeled dataset into training and validation sets based on a ratio\n",
        "# if train_size is 0.3, then training is 70% of the dataset\n",
        "df_training, df_validation = train_test_split(df_labeled, train_size = TRAININGDATASPLIT_RATIO)\n",
        "\n",
        "# pandas.DataFrame.shape is a tuple\n",
        "# https://www.programiz.com/python-programming/methods/tuple\n",
        "total_dfTraining = df_training.shape[0]\n",
        "total_dfValidation = df_validation.shape[0]\n",
        "\n",
        "\n",
        "print(\"Labeled Data Allocation\")\n",
        "fmt = '{:<4} {:<2} {:<15} '\n",
        "print(fmt.format(\"Train :\", \"\", total_dfTraining))\n",
        "print(fmt.format(\"Valid :\", \"\", total_dfValidation))\n",
        "\n",
        "# ref https://github.com/IResearchAI/Tobacco_Leaves_Classification_CNN/blob/master/Tob_main1.ipynb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-nhz8jlmU4N",
        "colab_type": "text"
      },
      "source": [
        "## Preprocessing - Data Augmentation\n",
        "\n",
        "Data Augmentation is used to apply various transforms to existing labeled\n",
        "data to increase data diversity, without having to gather new data.\n",
        "\n",
        "* Keras' [ImageDataPrerocessing](https://keras.io/api/preprocessing/image/) API\n",
        "*   https://www.pyimagesearch.com/2019/07/08/keras-imagedatagenerator-and-data-augmentation/\n",
        "*   https://colab.research.google.com/github/google/eng-edu/blob/master/ml/pc/exercises/image_classification_part2.ipynb\n",
        "\n",
        "*   [In-depth on Data Augmentation](https://medium.com/mlait/image-data-augmentation-image-processing-in-tensorflow-part-2-b77237256df0)\n",
        "\n",
        "*  [Image Classification using data generators](https://mc.ai/tutorial-image-classification-with-keras-flow_from_directory-and-generators/)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRWD9xC16q9A",
        "colab_type": "text"
      },
      "source": [
        "Training Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASDECBIzmURr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# playing around here I guess\n",
        "dg_training = ImageDataGenerator(\n",
        "    rotation_range = 15,\n",
        "    rescale = 1./255,\n",
        "    shear_range = 0.2,\n",
        "    zoom_range = 0.2,\n",
        "    horizontal_flip= True,\n",
        "    fill_mode = 'nearest',\n",
        "    width_shift_range = 0.1,\n",
        "    height_shift_range = 0.1\n",
        ")\n",
        "\n",
        "# Returns \n",
        "# A DataFrameIterator yielding tuples of (x, y) where x is a numpy array \n",
        "# containing a batch of images with shape (batch_size, *target_size, channels) \n",
        "# and y is a numpy array of corresponding labels.\n",
        "\n",
        "train_generator = dg_training.flow_from_dataframe(\n",
        "    df_training, \n",
        "    directory = trainingData_dir, \n",
        "    x_col = 'imageName',\n",
        "    y_col = 'petType',\n",
        "    # class_mode = 'binary',\n",
        "    class_mode = 'categorical',\n",
        "    target_size = (IMAGE_WIDTH, IMAGE_LENGTH),\n",
        "    batch_size = BATCH_SIZE,\n",
        "    seed = SEED\n",
        ")\n",
        "\n",
        "print(\"Visualized Example of Generator.\")\n",
        "def plotImages(images_arr):\n",
        "    fig, axes = plt.subplots(1, 6, figsize=(20,20))\n",
        "    axes = axes.flatten()\n",
        "    for img, ax in zip( images_arr, axes):\n",
        "        ax.imshow(img)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# TODO have it return random images ?\n",
        "augmented_images = [train_generator[0][0][0] for i in range(6)]\n",
        "plotImages(augmented_images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_K4roal1WKx",
        "colab_type": "text"
      },
      "source": [
        "Validation Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6P0YSN8dblH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# same stuff but for validation data\n",
        "# playing around here I guess\n",
        "dg_validation = ImageDataGenerator(\n",
        "    rotation_range = 15,\n",
        "    rescale = 1./255,\n",
        "    shear_range = 0.2,\n",
        "    zoom_range = 0.2,\n",
        "    horizontal_flip= True,\n",
        "    fill_mode = 'nearest',\n",
        "    width_shift_range = 0.1,\n",
        "    height_shift_range = 0.1\n",
        ")\n",
        "\n",
        "# Returns \n",
        "# A DataFrameIterator yielding tuples of (x, y) where x is a numpy array \n",
        "# containing a batch of images with shape (batch_size, *target_size, channels) \n",
        "# and y is a numpy array of corresponding labels.\n",
        "\n",
        "valid_generator = dg_validation.flow_from_dataframe(\n",
        "    df_validation, \n",
        "    directory = trainingData_dir, \n",
        "    x_col = 'imageName',\n",
        "    y_col = 'petType',\n",
        "    # class_mode = 'binary',\n",
        "    class_mode = 'categorical',\n",
        "    target_size = (IMAGE_WIDTH, IMAGE_LENGTH),\n",
        "    batch_size = BATCH_SIZE,\n",
        "    seed = SEED\n",
        ")\n",
        "\n",
        "# print(\"Visualized Example of Generator.\")\n",
        "# def plotImages(images_arr):\n",
        "#     fig, axes = plt.subplots(1, 6, figsize=(20,20))\n",
        "#     axes = axes.flatten()\n",
        "#     for img, ax in zip( images_arr, axes):\n",
        "#         ax.imshow(img)\n",
        "#     plt.tight_layout()\n",
        "#     plt.show()\n",
        "\n",
        "# # TODO have it return random images\n",
        "# augmented_images = [valid_generator[0][0][0] for i in range(6)]\n",
        "# plotImages(augmented_images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_VHvEZgUZtl",
        "colab_type": "text"
      },
      "source": [
        "## Test Data (TODO)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCRIhn8gUZi5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Qaz6xWISRGn",
        "colab_type": "text"
      },
      "source": [
        "# Model Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSxoFhx0RgUB",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "*   [MobileNet from scratch](https://github.com/IResearchAI/Tobacco_Leaves_Classification_CNN/blob/master/Tob_main2.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EqPFpFCRfo0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# mobile = keras.applications.mobilenet.MobileNet()\n",
        "\n",
        "# TODO : THIS IS JUST A COPY PASTE from the link above. Might have to adjust/change\n",
        "\n",
        "def mobilenetx(input_shape, n_classes):\n",
        "  \n",
        "  # TODO modify this to include pointwise ?\n",
        "  # maybe use this ? https://keras.io/api/layers/convolution_layers/separable_convolution2d/\n",
        "  def mobilenet_block(x, f, s=1):\n",
        "    x = DepthwiseConv2D(3, strides=s, padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "    \n",
        "    x = Conv2D(f, 1, strides=1, padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "    return x\n",
        "    \n",
        "    \n",
        "  input = Input(input_shape)\n",
        "\n",
        "  x = Conv2D(32, 3, strides=2, padding='same')(input)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = ReLU()(x)\n",
        "\n",
        "  x = mobilenet_block(x, 64)\n",
        "  x = mobilenet_block(x, 128, 2)\n",
        "  x = mobilenet_block(x, 128)\n",
        "\n",
        "  x = mobilenet_block(x, 256, 2)\n",
        "  x = mobilenet_block(x, 256)\n",
        "\n",
        "  x = mobilenet_block(x, 512, 2)\n",
        "  for _ in range(5):\n",
        "    x = mobilenet_block(x, 512)\n",
        "\n",
        "  x = mobilenet_block(x, 1024, 2)\n",
        "  x = mobilenet_block(x, 1024)\n",
        "  \n",
        "  x = GlobalAvgPool2D()(x)\n",
        "  \n",
        "  output = Dense(n_classes, activation='softmax')(x)\n",
        "  \n",
        "  model = Model(input, output)\n",
        "  return model\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3eJG0Fd0eTw",
        "colab_type": "text"
      },
      "source": [
        "We want to [adjust learning rate](https://machinelearningmastery.com/learning-rate-for-deep-learning-neural-networks/) when accuracy reaches certain thresholds.\n",
        "\n",
        "By using [stochastic gradient descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent) algoritm, it estimates the error gradient for the current training model and updates the weights using backpropagation.\n",
        "\n",
        "API CALLS\n",
        "\n",
        "* Keras [SGD](https://keras.io/api/optimizers/sgd/)\n",
        "* [EarlyStopping](https://machinelearningmastery.com/early-stopping-to-avoid-overtraining-neural-network-models/) : Used to reduce [overfitting](https://en.wikipedia.org/wiki/Overfitting) the training dataset and improve generalization\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4OnHQ-cbLvP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Overfitting setup\n",
        "# from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "earlystop = EarlyStopping(patience=10)\n",
        "\n",
        "# learning rate change after accuracy stalls\n",
        "learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n",
        "                                            patience=2, \n",
        "                                            verbose=1, \n",
        "                                            factor=0.5, \n",
        "                                            min_lr=0.00001)\n",
        "\n",
        "callbacks = [earlystop, learning_rate_reduction]\n",
        "\n",
        "# TODO : I think we might need to actually use an optimizer\n",
        "#  ref :  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UJu578wUKsW",
        "colab_type": "text"
      },
      "source": [
        "## Summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-9dLxJEUKgr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_shape=(IMAGE_WIDTH, IMAGE_LENGTH, IMAGE_NUMCHANNELS);\n",
        "n_classes = 2;\n",
        "# manual mobilenet implementation\n",
        "model = mobilenetx(input_shape, n_classes);\n",
        "model.summary()\n",
        "\n",
        "\n",
        "# keras' mobilenet v2 implementation....not implemented because I keep getting inf loss\n",
        "# tf.keras.applications.MobileNetV2(\n",
        "#     input_shape=None,\n",
        "#     alpha=1.0,\n",
        "#     include_top=True,\n",
        "#     weights=\"imagenet\",\n",
        "#     input_tensor=None,\n",
        "#     pooling=None,\n",
        "#     classes=1000,\n",
        "#     classifier_activation=\"softmax\",\n",
        "#     **kwargs\n",
        "# )\n",
        "\n",
        "# keras' own Mobilenetv2 implementation...\n",
        "# model_keras = keras.applications.MobileNet(input_shape=IMAGE_SIZE, classes = 2, classifier_activation = \"softmax\")\n",
        "base_model = MobileNetV2(input_shape=input_shape,\n",
        "                                      include_top=False,\n",
        "                                      weights='imagenet')\n",
        "print(\"model.layers\", len(base_model.layers)) #155\n",
        "\n",
        "#Freeze the convolutional base\n",
        "for layer in base_model.layers[:100]:\n",
        "    layer.trainable = False\n",
        "\n",
        "top_model = Sequential()\n",
        "top_model.add(Flatten(input_shape=base_model.output_shape[1:]))\n",
        "top_model.add(Dense(256, activation='relu'))\n",
        "top_model.add(Dropout(0.5))\n",
        "top_model.add(Dense(2, activation='sigmoid'))\n",
        "\n",
        "model_keras = Sequential()\n",
        "model_keras.add(base_model)\n",
        "model_keras.add(top_model)\n",
        "\n",
        "base_model.summary()\n",
        "top_model.summary()\n",
        "model_keras.summary()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MC8gLyVbT0Xr",
        "colab_type": "text"
      },
      "source": [
        "## Visualizations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lv-jyLLgUwL0",
        "colab_type": "text"
      },
      "source": [
        "*   Architecture Visualization : https://github.com/IResearchAI/Tobacco_Leaves_Classification_CNN/blob/master/Tob_main4.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FZ6Hre7T0GJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO Make this better\n",
        "\n",
        "from IPython.display import SVG\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "from keras.utils import plot_model\n",
        "\n",
        "plot_model(model,rankdir='TB')\n",
        "\n",
        "plot_model(model_keras,rankdir='TB')\n",
        "\n",
        "# SVG(model_to_dot(model).create(prog='dot', format='svg'))\n",
        "# def svg_to_fixed_width_html_image(svg, width=\"100%\"):\n",
        "#     text = _html_template.format(width, base64.b64encode(svg))\n",
        "#     return HTML(text)\n",
        "\n",
        "# svg_to_fixed_width_html_image(output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bThDO6Eabg_S",
        "colab_type": "text"
      },
      "source": [
        "# Train the Model\n",
        "\n",
        "* [How to interpret “loss” and “accuracy” for a machine learning model](https://stackoverflow.com/questions/34518656/how-to-interpret-loss-and-accuracy-for-a-machine-learning-model)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KcZQGXplbfBA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from keras import optimizers\n",
        "# from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# # check keras version\n",
        "# print(tf.keras.__version__)\n",
        "# print(keras.__version__)\n",
        "\n",
        "\n",
        "# if not os.path.exists('model'):\n",
        "#     os.makedirs(\"model\")\n",
        "# learningRate = 1e-4\n",
        "# compile the model with a SGD/momentum optimizer and a very slow learning rate.\n",
        "# model.compile(loss='categorical_crossentropy',\n",
        "#               optimizer=optimizers.SGD(lr=learningRate, momentum=0.9),\n",
        "#               #optimizer=optimizers.RMSprop(lr=learningRate),\n",
        "#               metrics=['accuracy'])\n",
        "\n",
        "# using save self best could be giving val_loss did not improve from inf ?\n",
        "# https://github.com/keras-team/keras/issues/12803\n",
        "# checkpointer = ModelCheckpoint(filepath='model/model.weights.best_MobileNetV2_1.hdf5', \n",
        "#                                verbose=1, save_best_only=True)\n",
        "\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['acc'])\n",
        "epochs = EPOCH_QUICK if QUICK_TRAIN else EPOCH_NORMAL\n",
        "\n",
        "# todo: depreciated and should change to Model.fit when we have time\n",
        "history = model.fit_generator(\n",
        "    train_generator,\n",
        "    # samples_per_epoch=TRAINING_SAMPLES, # depreciated from keras 1 api?\n",
        "    epochs = epochs,\n",
        "    validation_data = valid_generator,\n",
        "    validation_steps =  total_dfValidation // BATCH_SIZE,\n",
        "    steps_per_epoch = total_dfValidation // BATCH_SIZE,\n",
        "    # callbacks = [checkpointer] #if using keras Mobilenet\n",
        "    callbacks = callbacks\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7I12WXy-4aD",
        "colab_type": "text"
      },
      "source": [
        "## Feature Visualizations\n",
        "\n",
        "We want to visualize the model's trained filters\n",
        "\n",
        "* [Official Keras : Visualizing what convnets learn](https://keras.io/examples/vision/visualizing_what_convnets_learn/)\n",
        "* [How convolutional neural networks see the world](https://blog.keras.io/how-convolutional-neural-networks-see-the-world.html)\n",
        "* [Kernel Visualizations](https://github.com/raghakot/keras-vis)\n",
        "* [Tools to design/visualize NN architecture](https://github.com/ashishpatel26/Tools-to-Design-or-Visualize-Architecture-of-Neural-Network)\n",
        "* [Gradient Ascent](https://stackoverflow.com/questions/22594063/what-is-the-difference-between-gradient-descent-and-gradient-ascent)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gUsDQDI-2ga",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ref : https://keras.io/examples/vision/visualizing_what_convnets_learn/\n",
        "\n",
        "layer_name = str(model.get_layer(index = 1).name)\n",
        "\n",
        "\n",
        "# Set up a model that returns the activation values for our target layer\n",
        "layer = model.get_layer(name=layer_name)\n",
        "feature_extractor = Model(inputs=model.inputs, outputs=layer.output)\n",
        "\n",
        "# setup gradient ascent process\n",
        "def compute_loss(input_image, filter_index):\n",
        "    activation = feature_extractor(input_image)\n",
        "    # We avoid border artifacts by only involving non-border pixels in the loss.\n",
        "    filter_activation = activation[:, 2:-2, 2:-2, filter_index]\n",
        "    return tf.reduce_mean(filter_activation)\n",
        "\n",
        "@tf.function\n",
        "def gradient_ascent_step(img, filter_index, learning_rate):\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch(img)\n",
        "        loss = compute_loss(img, filter_index)\n",
        "    # Compute gradients.\n",
        "    grads = tape.gradient(loss, img)\n",
        "    # Normalize gradients.\n",
        "    grads = tf.math.l2_normalize(grads)\n",
        "    img += learning_rate * grads\n",
        "    return loss, img\n",
        "\n",
        "# setup end to end filter visualization loop\n",
        "\n",
        "def initialize_image():\n",
        "    # We start from a gray image with some random noise\n",
        "    img = tf.random.uniform((1, IMAGE_WIDTH, IMAGE_LENGTH, 3))\n",
        "    # ResNet50V2 expects inputs in the range [-1, +1].\n",
        "    # Here we scale our random inputs to [-0.125, +0.125]\n",
        "    return (img - 0.5) * 0.25\n",
        "\n",
        "\n",
        "def visualize_filter(filter_index):\n",
        "    # We run gradient ascent for 20 steps\n",
        "    iterations = 30\n",
        "    learning_rate = 10.0\n",
        "    img = initialize_image()\n",
        "    for iteration in range(iterations):\n",
        "        loss, img = gradient_ascent_step(img, filter_index, learning_rate)\n",
        "\n",
        "    # Decode the resulting input image\n",
        "    img = deprocess_image(img[0].numpy())\n",
        "    return loss, img\n",
        "\n",
        "\n",
        "def deprocess_image(img):\n",
        "    # Normalize array: center on 0., ensure variance is 0.15\n",
        "    img -= img.mean()\n",
        "    img /= img.std() + 1e-5\n",
        "    img *= 0.15\n",
        "\n",
        "    # Center crop\n",
        "    img = img[25:-25, 25:-25, :]\n",
        "\n",
        "    # Clip to [0, 1]\n",
        "    img += 0.5\n",
        "    img = numpy.clip(img, 0, 1)\n",
        "\n",
        "    # Convert to RGB array\n",
        "    img *= 255\n",
        "    img = numpy.clip(img, 0, 255).astype(\"uint8\")\n",
        "    return img\n",
        "\n",
        "from IPython.display import Image, display\n",
        "from keras import preprocessing\n",
        "\n",
        "loss, img = visualize_filter(0)\n",
        "preprocessing.image.save_img(\"0.png\", img)\n",
        "\n",
        "display(Image(\"0.png\"))\n",
        "\n",
        "# # Compute image inputs that maximize per-filter activations\n",
        "# # for the first 64 filters of our target layer\n",
        "# all_imgs = []\n",
        "\n",
        "# # TODO : it errors out after layer 32. To look into. Default = 64\n",
        "# # for filter_index in range(64):\n",
        "# for filter_index in range(31):\n",
        "#     print(\"Processing filter %d\" % (filter_index,))\n",
        "#     loss, img = visualize_filter(filter_index)\n",
        "#     all_imgs.append(img)\n",
        "\n",
        "# print(\"Count of all_imgs = \"+ str(len(all_imgs)))\n",
        "\n",
        "# # Build a black picture with enough space for\n",
        "# # our 8 x 8 filters of size 128 x 128, with a 5px margin in between\n",
        "# margin = 5\n",
        "# n = 8\n",
        "# cropped_width = IMAGE_WIDTH - 25 * 2\n",
        "# cropped_height = IMAGE_LENGTH - 25 * 2\n",
        "# width = n * cropped_width + (n - 1) * margin\n",
        "# height = n * cropped_height + (n - 1) * margin\n",
        "# stitched_filters = numpy.zeros((width, height, 3))\n",
        "\n",
        "# # Fill the picture with our saved filters\n",
        "# for i in range(n):\n",
        "#     for j in range(n):\n",
        "#         if ((i*n+j) < n):\n",
        "#           print(\"i * n + j :\" + str(i*n+j) + \" n: \" + str(n))\n",
        "#           img = all_imgs[i * n + j]\n",
        "#           stitched_filters[\n",
        "#               (cropped_width + margin) * i : (cropped_width + margin) * i + cropped_width,\n",
        "#               (cropped_height + margin) * j : (cropped_height + margin) * j\n",
        "#               + cropped_height,\n",
        "#               :,\n",
        "#           ] = img\n",
        "\n",
        "# preprocessing.image.save_img(\"stiched_filters.png\", stitched_filters)\n",
        "\n",
        "# from IPython.display import Image, display\n",
        "\n",
        "# display(Image(\"stiched_filters.png\"))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKObpz-QT33q",
        "colab_type": "text"
      },
      "source": [
        "# Model Accuracy and Loss graphs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MoXfmdvXT3ep",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## TODO \n",
        "\n",
        "# ref  : https://github.com/IResearchAI/Tobacco_Leaves_Classification_CNN/blob/master/Tob_main2.ipynb\n",
        "\n",
        "# this is outdated. revert to model.evaluate if possible\n",
        "loss, accuracy = model.evaluate_generator(valid_generator, \n",
        "                                          total_dfValidation // BATCH_SIZE, \n",
        "                                          workers=12, \n",
        "                                          use_multiprocessing = True)\n",
        "print(\"Test: accuracy = %f  ;  loss = %f \" % (accuracy, loss))\n",
        "\n",
        "\n",
        "\n",
        "def plot_model_history(model_history, accuracy='acc', val_accuracy='val_acc'):\n",
        "    fig, axs = plt.subplots(1,2,figsize=(50,15))\n",
        "    axs[0].plot(range(1,len(model_history.history[accuracy])+1),model_history.history[accuracy])\n",
        "    axs[0].plot(range(1,len(model_history.history[val_accuracy])+1),model_history.history[val_accuracy])\n",
        "    axs[0].set_title('Model Accuracy')\n",
        "    axs[0].set_ylabel('Accuracy')\n",
        "    axs[0].set_xlabel('Epoch')\n",
        "    axs[0].set_xticks(numpy.arange(1,len(model_history.history[accuracy])+1),len(model_history.history[accuracy])/10)\n",
        "    axs[0].legend(['train', 'val'], loc='best')\n",
        "    axs[1].plot(range(1,len(model_history.history['loss'])+1),model_history.history['loss'])\n",
        "    axs[1].plot(range(1,len(model_history.history['val_loss'])+1),model_history.history['val_loss'])\n",
        "    axs[1].set_title('Model Loss')\n",
        "    axs[1].set_ylabel('Loss')\n",
        "    axs[1].set_xlabel('Epoch')\n",
        "    axs[1].set_xticks(numpy.arange(1,len(model_history.history['loss'])+1),len(model_history.history['loss'])/10)\n",
        "    axs[1].legend(['train', 'val'], loc='best')\n",
        "    plt.show()\n",
        "    \n",
        "plot_model_history(history)\n",
        "\n",
        "# acc = history.history['acc']\n",
        "# val_acc = history.history['val_acc']\n",
        "# loss = history.history['loss']\n",
        "# val_loss = history.history['val_loss']\n",
        "# epochs = range(1, len(acc) + 1)\n",
        "# plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "# plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "# plt.title('Training and validation accuracy')\n",
        "# plt.legend()\n",
        "# plt.figure()\n",
        "# plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "# plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "# plt.title('Training and validation loss')\n",
        "# plt.legend()\n",
        "# plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moKsupPPT8Ih",
        "colab_type": "text"
      },
      "source": [
        "# Confusion Matrix\n",
        "\n",
        "A [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix) is a table that is often used to describe the performance of a classification model (or “classifier”) on a set of test data for which the true values are known. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zw_qcFreT7_-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}